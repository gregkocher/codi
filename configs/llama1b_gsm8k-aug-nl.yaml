# Model Arguments
model_args:
  model_name_or_path: "meta-llama/Llama-3.2-1B-Instruct"
  lora_r: 128
  lora_alpha: 32
  lora_init: true
  token: null
  full_precision: true

# Data Arguments
data_args:
  data_names:
    - "icot_full"
    # - "strategy"
    # - "commonsense"
  max_samples: null

# Wandb Arguments
wandb:
  project: "codi"
  run_name: "codi_llama1b_gsm8k-answer-empty"

# Hub Arguments
hub:
  push_to_hub: true
  account: "bcywinski"
  private_repo: false

# Training Arguments
training_args:
  output_dir: "./models/codi_llama1b_gsm8k-answer-empty"
  expt_name: "codi_llama1b_gsm8k-answer-empty"
  logging_dir: "./models/codi_llama1b_gsm8k-answer-empty/logs"
  logging_steps: 10
  seed: 11
  model_max_length: 512
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1
  bf16: true
  max_steps: -1
  num_train_epochs: 1
  learning_rate: 8e-4
  max_grad_norm: 2.0
  save_strategy: "epoch"
  save_total_limit: 1
  save_safetensors: false
  weight_decay: 0.1
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  do_train: true
  report_to: "wandb"
  num_latent: 6
  logging_strategy: "steps"
  use_prj: true
  prj_dim: 2048
  prj_dropout: 0.0
  distill_loss_div_std: true
  exp_mode: false
  exp_data_num: 200
  remove_eos: true
  distill_loss_factor: 20
  ref_loss_factor: 1.0
  print_ref_model_stats: false
  print_loss: false
  max_token_num: 256
  dataloader_num_workers: 4
  ddp_find_unused_parameters: false
  sft_loss_factor: 1.0
  include_last_cot: true
  ce_loss_factor: 1.0
