# Model Arguments
model_args:
  model_name_or_path: "meta-llama/Llama-3.2-1B-Instruct"
  lora_r: 128
  lora_alpha: 32
  lora_init: true
  token: null
  full_precision: true
  attn_implementation: "flash_attention_2"

# Data Arguments
data_args:
  data_name: "icot_full"
  max_samples: null

# Wandb Arguments
wandb:
  project: "codi"
  run_name: "llama1b_gsm8k-aug-nl"

# Hub Arguments
hub:
  push_to_hub: true
  account: "bcywinski"
  private_repo: false

# Training Arguments
training_args:
  output_dir: "./models/llama1b_gsm8k-aug-nl"
  expt_name: "llama1b_gsm8k-aug-nl"
  logging_dir: "./models/llama1b_gsm8k-aug-nl/logs"
  logging_steps: 10
  seed: 11
  model_max_length: 512
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1
  bf16: true
  max_steps: -1
  num_train_epochs: 3
  learning_rate: 8e-4
  max_grad_norm: 2.0
  save_strategy: "no"
  save_total_limit: 1
  save_safetensors: false
  weight_decay: 0.1
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  do_train: true
  report_to: "wandb"
  num_latent: 6
  logging_strategy: "steps"
  use_prj: true
  prj_dim: 2048
  prj_dropout: 0.0
  distill_loss_div_std: true
  exp_mode: false
  exp_data_num: 200
  remove_eos: true
  distill_loss_factor: 20
  print_ref_model_stats: true
  max_token_num: 256
