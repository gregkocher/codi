# Model Arguments
model_args:
  model_name_or_path: "gpt2"
  lora_r: 128
  lora_alpha: 32
  lora_init: true
  token: null
  full_precision: true
  ckpt_dir: "/workspace/projects/codi/models/codi_gpt2_orig"

# Data Arguments
data_args:
  data_names: "gsm8k"  # Options: "gsm8k", "gsm-hard", "multi-arith", "svamp", "commonsense"
  batch_size: 128

# Training Arguments (for inference)
training_args:
  model_max_length: 512
  bf16: true
  greedy: true
  num_latent: 6
  use_prj: true
  prj_dim: 768
  prj_no_ln: false
  prj_dropout: 0.0
  inf_latent_iterations: 6
  inf_num_iterations: 1
  remove_eos: true
  use_lora: true
