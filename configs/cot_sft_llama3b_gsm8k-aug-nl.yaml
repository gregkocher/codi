# Model Arguments
model_args:
  model_name_or_path: "meta-llama/Llama-3.2-3B-Instruct"
  lora_r: 128
  lora_alpha: 32
  lora_init: true
  token: null
  full_precision: true
  attn_implementation: "flash_attention_2"

# Data Arguments
data_args:
  data_names:
    - "icot_full"
    - "strategy"
    - "commonsense"
  max_samples: null

# Wandb Arguments
wandb:
  project: "codi"
  run_name: "cot_sft_llama3b_gsm8k-strategyqa-commonsense"

# Hub Arguments
hub:
  push_to_hub: true
  account: "bcywinski"
  private_repo: false

# Training Arguments
training_args:
  output_dir: "./models/cot_sft_llama3b_gsm8k-strategyqa-commonsense"
  expt_name: "cot_sft_llama3b_gsm8k-strategyqa-commonsense"
  logging_dir: "./models/cot_sft_llama3b_gsm8k-strategyqa-commonsense/logs"
  logging_steps: 10
  seed: 11
  model_max_length: 512
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1
  bf16: true
  max_steps: -1
  num_train_epochs: 3
  learning_rate: 8e-4
  max_grad_norm: 2.0
  save_strategy: "epoch"
  weight_decay: 0.1
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  report_to: "wandb"
  logging_strategy: "steps"
  dataloader_num_workers: 4
  ddp_find_unused_parameters: false
